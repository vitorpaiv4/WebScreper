# Web Scraper em Ruby üï∑Ô∏è

Um scraper modular para extrair dados de sites, desenvolvido em Ruby com HTTParty e Nokogiri.  
**Funcionalidades**: requisi√ß√µes HTTP, extra√ß√£o de dados via seletores CSS, exporta√ß√£o para CSV e logs detalhados.

---

## üöÄ Como Usar

### Pr√©-requisitos
- Ruby 3.0+ instalado
- Gerenciador de gems (Bundler recomendado)

### Instala√ß√£o
1. Clone o reposit√≥rio:
   ```bash
   git clone https://github.com/seu-usuario/web-scraper-ruby.git
   cd web-scraper-ruby
   ```
2. Instale as depend√™ncias:
   ```bash
   bundle install
   ```

üìã **Configura√ß√£o**  
Arquivo `config/config.yml`  
Configure URLs e seletores CSS:
```yaml
urls:
  - https://exemplo.com/produtos
  - https://exemplo.com/blog

selectors:
  produto: '.produto'       # Container de cada item
  nome: '.nome-produto'     # Nome do produto
  preco: '.preco'           # Pre√ßo
  link: 'a[href]'           # Link do produto

settings:
  user_agent: "Mozilla/5.0 (Ruby Scraper)"
  delay: 2                  # Delay entre requisi√ß√µes (segundos)
```

‚ñ∂Ô∏è **Execu√ß√£o**  
**Scraping B√°sico**  
```bash
ruby run.rb
```
Os dados ser√£o salvos em `data/produtos.csv`.

**Testes (RSpec)**  
```bash
rspec spec/
```

üóÇÔ∏è **Estrutura do Projeto**
```
web-scraper-ruby/
‚îú‚îÄ‚îÄ lib/                  # C√≥digo-fonte
‚îÇ   ‚îú‚îÄ‚îÄ scraper.rb        # L√≥gica principal do scraping
‚îÇ   ‚îú‚îÄ‚îÄ data_exporter.rb  # Exporta√ß√£o para CSV/JSON
‚îÇ   ‚îî‚îÄ‚îÄ utils.rb          # Utilit√°rios (logs, headers)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yml        # Configura√ß√µes do site
‚îú‚îÄ‚îÄ spec/                 # Testes
‚îÇ   ‚îú‚îÄ‚îÄ scraper_spec.rb
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/         # HTMLs mockados para testes
‚îú‚îÄ‚îÄ data/                 # Dados extra√≠dos (CSV/JSON)
‚îú‚îÄ‚îÄ logs/                 # Logs de execu√ß√£o
‚îú‚îÄ‚îÄ run.rb                # Script de execu√ß√£o
‚îú‚îÄ‚îÄ Gemfile               # Depend√™ncias
‚îî‚îÄ‚îÄ README.md
```

üîß **Depend√™ncias Principais**
- HTTParty: Requisi√ß√µes HTTP.
- Nokogiri: Parseamento de HTML/XML.
- RSpec: Testes unit√°rios.
- WebMock: Mock de requisi√ß√µes HTTP.

Lista completa em `Gemfile`.

‚ö†Ô∏è **Boas Pr√°ticas**
- Respeite o `robots.txt` do site alvo.
- Adicione delay entre requisi√ß√µes para evitar bloqueios.
- Use headers personalizados (ex: User-Agent) para simular navegadores reais.
- Para sites com JavaScript, substitua HTTParty por Ferrum ou Selenium.

üìÑ **Exemplo de C√≥digo (scraper.rb)**
```ruby
require 'httparty'
require 'yaml'

class Scraper
  def initialize
    config = YAML.load_file('config/config.yml')
    @url = config['urls'].first
    @selectors = config['selectors']
  end

  def scrape
    response = HTTParty.get(@url, headers: { "User-Agent" => "Scraper" })
    doc = Nokogiri::HTML(response.body)
    
    doc.css(@selectors['produto']).map do |produto|
      {
        nome: produto.css(@selectors['nome']).text.strip,
        preco: produto.css(@selectors['preco']).text.strip,
        link: produto.css(@selectors['link']).first['href']
      }
    end
  end
end
```

ü§ù **Como Contribuir**
- Fa√ßa um fork do projeto.
- Crie uma branch (git checkout -b feature/nova-funcionalidade).
- Commit suas mudan√ßas (git commit -m 'Adiciona nova funcionalidade').
- Push para a branch (git push origin feature/nova-funcionalidade).
- Abra um Pull Request.

üìù **Licen√ßa**
Distribu√≠do sob a licen√ßa MIT. Veja LICENSE para mais detalhes.

Nota: Use este projeto apenas em sites que permitam scraping e para fins √©ticos. üü¢

---

### **Personaliza√ß√£o:**
- Substitua `https://exemplo.com` pela URL do seu site alvo.
- Ajuste os seletores CSS em `config.yml` conforme a estrutura do site.
- Adicione mais funcionalidades (ex: autentica√ß√£o, pagina√ß√£o) conforme necess√°rio.
